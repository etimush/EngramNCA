<!--
  Copyright 2018 The Distill Template Authors

  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

       http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
-->

<!DOCTYPE html>
<!--[if IE lte 8]><script src="http://html5shiv.googlecode.com/svn/trunk/html5.js" defer></script><![endif]-->
<head>
  <script src="./template.v2.js"></script>

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel = "stylesheet" href="style.css">
  <meta charset="utf8">
</head>

<body>

  <d-front-matter>
    <script id='distill-front-matter' type="text/json">{
    "title": "EngramNCA: a Neural Cellular Automaton Model of Memory Transfer",
    "description": "Although \" extremely useful for visualizing high-dimensional data, t-SNE plots can sometimes be mysterious or misleading.",
    "published": "March 1, 2025",


    "authors": [
      {
        "author":"Etienne Guichard",
        "authorURL":"",
        "affiliations": [{"name": "Østfold University College", "url": "https://www.hiof.no/english/"}]
      },

      {
        "author":"Felix Simon Reimers",
        "authorURL":"https://www.hiof.no/iio/english/people/aca/felixsr/index.html",
        "affiliations": [
          {"name": "Østfold University College", "url": "https://www.hiof.no/english/"}
        ]
      },

      {
        "author":"Mia-Katrin Kvalsund",
        "authorURL":"https://www.mn.uio.no/fysikk/english/people/aca/mkkvalsu",
        "affiliations": [
          {"name": "University of Oslo", "url": "https://www.uio.no/english/"}
        ]
      },
      {
        "author":"Mikkel Lepperød",
        "authorURL":"https://lepmik.github.io/",
        "affiliations": [
          {"name": "Simula Research Laboratory, University of Oslo", "url": "https://www.simula.no/"}
        ]
      },
      {
        "author":"Stefano Nichele*",
        "authorURL":"https://www.nichele.eu/",
        "affiliations": [{"name": "Østfold University College, Oslo Metropolitan University", "url": "https://www.hiof.no/english/"}]
      }


    ],
    "_url": "https://etimush.github.io/EngramNCA/",
    "journal": "Github Preprint",
    "katex": {
      "delimiters": [
        {"left": "$$", "right": "$$", "display": false}
      ]
    }
  }</script>
  </d-front-matter>
  <d-title>
    <figure style="grid-column: page; margin: 1rem 0;"><video loop autoplay playsinline muted src="figures/banner.mp4"
        style="width:100%; border: 1px solid rgba(0, 0, 0, 0.2);" /></figure>
    <p><b>Left:</b> GeneCA trained to grow simple polygons through immutable gene encodings. These polygons are stable in time, exhibit individuality (strong morphological boundaries), and their genes can be combined to produce new out-of-training polygons.
      <b>Center:</b> GenePropCA trained to grow a lizard morphology by exploiting the gene embeddings of the GeneCA (left). The GeneCA cannot modify its own gene embedding, and the GenePropCA cannot modify the visible channels, yet the GenePropCA can learn to exploit the GeneCA gene embeddings and force it to draw a target morphology.
    <b>Right:</b> By adding immutable genes to the GenePropCA that neither it nor the GeneCA can modify, we can encode multiple target morphologies, where the GenePropCA can exploit the GeneCA genes to grow different morphologies.</p>
  </d-title>
  <d-byline></d-byline>

  <d-article margib-bottom: 0>

    <p style="color:#696969;font-size:13px;">Current version: draft v.0.1, Feb 2025 <br> *Corresponding author <a href="stefano.nichele@hiof.no" target="_blank">(stefano.nichele@hiof.no)</a> <br> Paper Repository: <a href="https://github.com/etimush/MemoryNCA" target="_blank">GitHub</a> <br>PDF Preprint: Will be added here <br> Published article: will be added here <br> Google Colab: <a target="_blank" href="https://colab.research.google.com/github/etimush/MemoryNCA/blob/main/GeneCA.ipynb">
  <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/>
</a></p>

  </d-article>

%% article.html

  <d-appendix>

    <d-bibliography src="bibliography.bib"></d-bibliography>



  </d-appendix>

  <d-appendix>
    <h3>Appendix 1</h3>
    <p>This sections details the preliminary experiments conducted to determine the deterioration in NCA image reconstruction quality as channels are privatized. We measured the loss curves for reconstruction of a lizard emoji, alongside the regrowth capabilities of the NCA to damage. For all experiments the training was repeated once on a set seed for each privatization level, the regrowth experiments where repeated one-hundred times with the damage locations being fixed and identical for each privatization level.
    </p>
    <p>For these experiments we tested three different NCA types:</p>
    <ul style="margin-top: 0">
      <li><b>DummyVCA</b>: Where the sensing kernels channel dimension match the unmasked NCA channels. However, any kernel involved in sensing neighbourhoods (such as sobel filters) would receive a dummy vector of all zeros for in lieu of the privatized channels.</li>
      <li><b>MaskedCA</b>: Where the sensing kernels channel dimension match the unmasked NCA channels. However, the channels of the output tensor corresponding to the privatized channels where masked with zeros after the sensing convolution.  </li>
      <li><b>ReducedCA</b>: Where the sensing kernels channel dimension where reduced to not include the privatized channels, and a truncated state excluding the privatized channels is passed to the sensing kernels. The state was later re-expanded with zeros t mathc the unmasked NCA channel dimensions.</li>
    </ul>


    <p> The loss function used for training and measuring the reconstruction was the PixelwiseMSE:</p>

    <d-math style="text-align: center; margin: 2rem 0;">

      PixelWiseMSE = \frac{1}{H \times W \times C} \sum_{i=0}^{H} \sum_{j=0}^{W} \sum_{k=0}^{C} \left( I(i, j, k) - \hat{I}(i, j, k) \right)^2

    </d-math>

    <p>Where <d-math>H,W,C</d-math> are the dimension of the image, <d-math>I</d-math> is the reference image, and <d-math>\hat{I}</d-math> is the final state of the NCA.</p>

    <h2>Preliminary experiment results</h2>

    <p><a href="#figure-interComparison" class="figure-link" data-target="figure-interComparison"></a> shows the loss curves of the three models with different levels of privatization (0,4,8,12, left to right). For privatization levels 0 and 4 the three models are comparable, while at privatization level 8 the DummyVCA performed considerably worse.Finally, the ReducedCA performed best at privatization level 12. From privatization level 0 to 4 there seems to be no discernible change in the loss curves, afterward however, as more channels get privatized the models struggle to learn. </p>

    <figure style="text-align: center; margin: 4rem 0;" id="figure-interComparison"><img playsinline src="figures/Inter_model_comparison.png";
          style="width:100%; border: 0px solid rgba(0, 0, 0, 0.2);" /></figure><figcaption>Comparison of the loss curves of the three models with different levels of privatization (0,4,8,12, left to right). For privatization levels 0 and 4 the three models are comparable, while at privatization level 8 the DummyVCA performed considerably worse.Finally, the ReducedCA performed best at privatization level 12.</figcaption>
    <p><br></p>
    <p><a href="#figure-loss" class="figure-link" data-target="figure-loss"></a> shows how the reconstruction quality (the mean final loss, on a running average of 10) on the lizard emoji decreases as channels are privatized. The data is noisy, thus no proper comparison can be done between the three models, they can be considered to act the same. </p>

    <figure style="text-align: center; margin: 4rem 0;" id="figure-loss"><img playsinline src="figures/Inter_model_loss_fine.png";
          style="width:100%; border: 0px solid rgba(0, 0, 0, 0.2);" /></figure><figcaption>Mean final loss as compared to number of channels privatized for each of the three models. As channels are privatized, it can be seen that the reconstruction quality of the original image (mean final loss) decreases. </figcaption>

    <p><br></p>
    <p><a href="#figure-regrowth" class="figure-link" data-target="figure-regrowth"></a> shows the mean regrowth loss of the three models as they attempt to reconstruct the damaged lizard. As with the mean final loss, regrowth quality decreases as the number of channels are privatized. The data is once again noisy, even more so here. This is most likely due to the asynchronous nature of NCAs.  </p>


    <figure style="text-align: center; margin: 4rem 0;" id="figure-regrowth"><img playsinline src="figures/Regrowth_loss.png";
          style="width:100%; border: 0px solid rgba(0, 0, 0, 0.2);" /></figure><figcaption>Mean regrowth loss as compared to number of channels privatized for each of the three models. As channels are privatized, it can be seen that the NCA struggles to regrow back to the original image.</figcaption>

    <p><br></p>
    <p>From the results of the preliminary experiments, we concluded that there is indeed deterioration in the quality of image reconstruction from the NCA as channels are privatized. However, the choice of architecture does not play a big role in the level of performance loss. Thus, we chose to go with the MaskedCA version for channel reduction as it was the easiest to implement. </p>
  </d-appendix>

  <distill-appendix>
  <p>BibTeX Citation</p>
  <pre class="citation">
  @article{Guichard2025EngramNCA},
  title = {EngramNCA: a Neural Cellular Automaton Model of Molecular Memory Transfer},
  author = {Guichard, Etienne and Reimers, Felix and Kvalsund, Mia-Katrin and Lepper{\o}d, Mikkel and Nichele, Stefano},
  journal = {Github Preprint},
  year = {2025},
  url = { https://etimush.github.io/EngramNCA/}
  }</pre>
  </distill-appendix>

<script src="./figs.js"></script>
</body>
